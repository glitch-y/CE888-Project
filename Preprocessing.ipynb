{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwpL/vlObjdYdTzsEYSlB+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glitch-y/CE888-Project/blob/main/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYh7CIZk47j0",
        "outputId": "65dd7aa5-7b71-4720-b6da-a8f136555407",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install contractions\n",
        "!pip install emot"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/04/d5e0bb9f2cef5d15616ebf68087a725c5dbdd71bd422bcfb35d709f98ce7/contractions-0.0.48-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c7/61370d9e3c349478e89a5554c1e5d9658e1e3116cc4f2528f568909ebdf1/anyascii-0.1.7-py3-none-any.whl (260kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 266kB 7.8MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/92/b3c70b8cf2b76f7e3e8b7243d6f06f7cb3bab6ada237b1bce57604c5c519/pyahocorasick-1.4.1.tar.gz (321kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 327kB 13.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.1-cp37-cp37m-linux_x86_64.whl size=85266 sha256=69f11e3b452aac5e46d367e78224a351cb31f67079c305242c4d2346fd5387fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/ab/f7/cb39270df8f6126f3dd4c33d302357167086db460968cfc80c\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: anyascii, pyahocorasick, textsearch, contractions\n",
            "Successfully installed anyascii-0.1.7 contractions-0.0.48 pyahocorasick-1.4.1 textsearch-0.0.21\n",
            "Collecting emot\n",
            "  Downloading https://files.pythonhosted.org/packages/49/07/20001ade19873de611b7b66a4d5e5aabbf190d65abea337d5deeaa2bc3de/emot-2.1-py3-none-any.whl\n",
            "Installing collected packages: emot\n",
            "Successfully installed emot-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOm5O4dtSmsw"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDcMGUXmMUlr"
      },
      "source": [
        "#Import modules\n",
        "import html #import 'html' module to clean html elements such as '&amp;, &lt' etc.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import contractions #import 'contractions' module to expand linguistic contactions (e.g. it's = it is)\n",
        "from emot import UNICODE_EMO #import emoji dictionary to transform emojis into text\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MKar_aiSqGJ"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2vc0edv2eYr"
      },
      "source": [
        "#Import files for the 'Emotion' task\n",
        "data_emotion_test = pd.read_csv(f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/test_text.txt\", \n",
        "                                delimiter='\\t', dtype=str, header= None)\n",
        "data_emotion_test_labels = pd.read_csv(f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/test_labels.txt\", \n",
        "                                      delimiter='\\t', dtype=str, header= None)\n",
        "data_emotion_mapping = pd.read_csv(f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/mapping.txt\", \n",
        "                                      delimiter='\\t', dtype=str, header= None)\n",
        "data_emotion_train = pd.read_csv(f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/train_text.txt\", \n",
        "                                      delimiter='\\t', dtype=str, header= None)\n",
        "data_emotion_train_labels = pd.read_csv(f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/train_labels.txt\", \n",
        "                                      delimiter='\\t', dtype=str, header= None)\n",
        "data_emotion_val = pd.read_csv(f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/val_text.txt\", \n",
        "                                      delimiter='\\t', dtype=str, header= None)\n",
        "data_emotion_val_labels = pd.read_csv(f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/val_labels.txt\", \n",
        "                                      delimiter='\\t', dtype=str, header= None)\n",
        "\n",
        "#Assign column names for Emotion datasets\n",
        "data_emotion_test.columns =['content']\n",
        "data_emotion_test_labels.columns =['labels']\n",
        "data_emotion_mapping.columns =['labels','mapping']\n",
        "data_emotion_train.columns =['content']\n",
        "data_emotion_train_labels.columns =['labels']\n",
        "data_emotion_val.columns =['content']\n",
        "data_emotion_val_labels =['labels'] "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7vYzjZOS4wt"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOoEHhzEw9YE"
      },
      "source": [
        "##Replace misspelled words\n",
        "\n",
        "Typos are common in text data and the scripts below create a dictionary of commonly mispelled words and applies it against the 3 'text' sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fse8feXLnQUd",
        "outputId": "243973ff-2697-44e2-cc57-1ce5009379d0"
      },
      "source": [
        "#Import misspelling data as dictionary\n",
        "misspell_data = pd.read_csv(\"https://raw.githubusercontent.com/glitch-y/CE888-Project/main/Misspelling.txt\",sep=\":\",names=[\"correction\",\"misspell\"])\n",
        "misspell_data.misspell = misspell_data.misspell.str.strip()\n",
        "misspell_data.misspell = misspell_data.misspell.str.split(\" \")\n",
        "misspell_data = misspell_data.explode(\"misspell\").reset_index(drop=True)\n",
        "misspell_data.drop_duplicates(\"misspell\",inplace=True)\n",
        "miss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))\n",
        "\n",
        "#Preview misspelling dictionary\n",
        "{v:miss_corr[v] for v in [list(miss_corr.keys())[k] for k in range(10)]}\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Steffen': 'Stephen',\n",
              " 'abilitey': 'ability',\n",
              " 'abouy': 'about',\n",
              " 'absorbtion': 'absorption',\n",
              " 'accidently': 'accidentally',\n",
              " 'accomodate': 'accommodate',\n",
              " 'nevade': 'Nevada',\n",
              " 'presbyterian': 'Presbyterian',\n",
              " 'rsx': 'RSX',\n",
              " 'susan': 'Susan'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk52rg9YZPZG"
      },
      "source": [
        "#Create misspelling correction function\n",
        "def misspelled_correction(x):\n",
        "    for i in x.split(): \n",
        "        if i in miss_corr.keys(): \n",
        "            x = x.replace(i, miss_corr[i]) \n",
        "    return x\n",
        "\n",
        "#Apply misspelling correction to text dataframes as new column\n",
        "data_emotion_test['content_clean'] = data_emotion_test.content.apply(lambda x : misspelled_correction(x).lower())\n",
        "data_emotion_train['content_clean'] = data_emotion_train.content.apply(lambda x : misspelled_correction(x).lower())\n",
        "data_emotion_val['content_clean'] = data_emotion_val.content.apply(lambda x : misspelled_correction(x).lower())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhrycsD0xlYx"
      },
      "source": [
        "##Replace abbreviated words\n",
        "\n",
        "Social media users normally use abbreviated text due to the fast nature of writing a post as well as certain limitations in terms of characters (a well-known aspect of Twitter. \n",
        "\n",
        "The script below create a dictionary of commonly known internet abbreviations and applies it against the 3 'text' data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1suRDHNoe4M",
        "outputId": "894437a7-ad01-4199-8a1b-d5e244bad3fa"
      },
      "source": [
        "#Abbreviated chat words conversion\n",
        "#Create Dictionary\n",
        "chat_dictionary = pd.read_csv(\"https://raw.githubusercontent.com/glitch-y/CE888-Project/main/SlangDictionary.csv\",dtype=str, names=[\"Slang\", \"Translation\"])\n",
        "chat_dictionary=chat_dictionary.apply(lambda x: x.str.lower())\n",
        "slang_corr = dict(zip(chat_dictionary.Slang, chat_dictionary.Translation))\n",
        "\n",
        "#Preview abbreviation dictionary\n",
        "{v:slang_corr[v] for v in [list(slang_corr.keys())[k] for k in range(10)]}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a.s.a.p.': 'as soon as possible',\n",
              " 'ama': 'ask me anything',\n",
              " 'asap': 'as soon as possible',\n",
              " 'atk': 'at the keyboard',\n",
              " 'atm': 'at the moment',\n",
              " 'bbl': 'be back later',\n",
              " 'bbs': 'be back soon',\n",
              " 'bc': 'because',\n",
              " 'bcs': 'because',\n",
              " 'bfn': 'bye for now'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "wO1rOaviyD4-",
        "outputId": "6440dbc3-bc0a-4e15-c1cd-d641eaeef275"
      },
      "source": [
        "#Create abbreviation replacement function\n",
        "def abbrev_replace(x):\n",
        "    for i in x.split(): \n",
        "        if i in slang_corr.keys(): \n",
        "            x = x.replace(i, slang_corr[i]) \n",
        "    return x\n",
        "\n",
        "#Apply misspelling correction to dataframe as new column\n",
        "data_emotion_test.content_clean = data_emotion_test.content_clean.apply(lambda x : abbrev_replace(x))\n",
        "data_emotion_train.content_clean = data_emotion_train.content_clean.apply(lambda x : abbrev_replace(x))\n",
        "data_emotion_val.content_clean = data_emotion_val.content_clean.apply(lambda x : abbrev_replace(x))\n",
        "\n",
        "#Check\n",
        "data_emotion_train.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>content_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>â€œWorry is a down payment on a problem you may ...</td>\n",
              "      <td>â€œworry is a down payment on a problem you may ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>My roommate: it's okay that we can't spell bec...</td>\n",
              "      <td>my roommate: it's okay that we can't spell bec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>No but that's so cute. Atsu was probably shy a...</td>\n",
              "      <td>no but that's so cute. atsu was probably shy a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Rooneys fucking untouchable isn't he? Been fuc...</td>\n",
              "      <td>rooneys fucking untouchable isn't he? been fuc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>it's pretty depressing when u hit pan on ur fa...</td>\n",
              "      <td>it's pretty depressing when u hit pan on  your...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content                                      content_clean\n",
              "0  â€œWorry is a down payment on a problem you may ...  â€œworry is a down payment on a problem you may ...\n",
              "1  My roommate: it's okay that we can't spell bec...  my roommate: it's okay that we can't spell bec...\n",
              "2  No but that's so cute. Atsu was probably shy a...  no but that's so cute. atsu was probably shy a...\n",
              "3  Rooneys fucking untouchable isn't he? Been fuc...  rooneys fucking untouchable isn't he? been fuc...\n",
              "4  it's pretty depressing when u hit pan on ur fa...  it's pretty depressing when u hit pan on  your..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0fTqmV5yha_"
      },
      "source": [
        "##Remove HTML elements\n",
        "\n",
        "Data scraped from various websites usually returns certain html elements such as '&amp;' for '&'\n",
        "\n",
        "The script below uses the 'html' module to clean the data of any such occurences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmSxVYV_0GYV",
        "outputId": "2e56b78c-f5bd-45b2-ad09-fb17f6a2dd71"
      },
      "source": [
        "#clean HTML charachters such as &amp;, &lt; etc using 'html' module\n",
        "data_emotion_test.content_clean = data_emotion_test.content_clean.apply(lambda x: html.unescape(x))\n",
        "data_emotion_train.content_clean = data_emotion_train.content_clean.apply(lambda x: html.unescape(x))\n",
        "data_emotion_val.content_clean = data_emotion_val.content_clean.apply(lambda x: html.unescape(x))\n",
        "\n",
        "#Check\n",
        "print(data_emotion_test.loc[[12]])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              content                                      content_clean\n",
            "12  Yes #depression &amp; #anxiety are real but so...  yes #depression & #anxiety are real but so is ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q5SXhUszBir"
      },
      "source": [
        "##Fix language contractions\n",
        "\n",
        "The script below uses the 'contractions' module to expand any language contractions such as 'let's' into 'let us' or 'it's' into 'it is'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIib9xFv5rkS",
        "outputId": "bdc98fdf-4ab2-4a97-bd5b-55326cbbe503"
      },
      "source": [
        "#fix contractions; i.e. 'It's' transforms into 'it is'\n",
        "data_emotion_test.content_clean = data_emotion_test.content_clean.apply(lambda x: contractions.fix(x))\n",
        "data_emotion_train.content_clean = data_emotion_train.content_clean.apply(lambda x: contractions.fix(x))\n",
        "data_emotion_val.content_clean = data_emotion_val.content_clean.apply(lambda x: contractions.fix(x))\n",
        "\n",
        "#Check\n",
        "print(data_emotion_test.loc[[54]])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              content                                      content_clean\n",
            "54  Let's start all over again.....\\n#feels #lover...  let us start all over again.....\\n#feels #love...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpGtSXcrz9aG"
      },
      "source": [
        "##Remove 'newlines' and replace '&' with 'and'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91OK_1gd77sz",
        "outputId": "3f65e7d8-a6a5-45e4-ae8c-c98683b1bbf8"
      },
      "source": [
        "#Remove newlines from data and replace '&' with 'and'\n",
        "data_emotion_test.content_clean = data_emotion_test.content_clean.replace(r'\\\\n',' ', regex=True)\n",
        "data_emotion_test.content_clean = data_emotion_test.content_clean.replace(r'&','and', regex=True)\n",
        "\n",
        "data_emotion_train.content_clean = data_emotion_train.content_clean.replace(r'\\\\n',' ', regex=True)\n",
        "data_emotion_train.content_clean = data_emotion_train.content_clean.replace(r'&','and', regex=True)\n",
        "\n",
        "data_emotion_val.content_clean = data_emotion_val.content_clean.replace(r'\\\\n',' ', regex=True)\n",
        "data_emotion_val.content_clean = data_emotion_val.content_clean.replace(r'&','and', regex=True)\n",
        "\n",
        "#Check\n",
        "print(data_emotion_test.loc[[34]])\n",
        "print(data_emotion_test.loc[[12]])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              content                                      content_clean\n",
            "34  @user -- can handle myself.\\n[Carl yelled back...  @user -- can handle myself. [carl yelled back ...\n",
            "                                              content                                      content_clean\n",
            "12  Yes #depression &amp; #anxiety are real but so...  yes #depression and #anxiety are real but so i...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybOwJ8qZ0nOd"
      },
      "source": [
        "##Convert emojis into text\n",
        "\n",
        "Emoji's describe a variety of emotions or objects which can help increase the accuracy of the algorithm. \n",
        "\n",
        "The script below uses the 'emot' module to lookup emoji's in the module dictionary and translate them into text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5M4iqt_EFII",
        "outputId": "ff9fc9f3-b063-49ef-a9ec-274748841b27"
      },
      "source": [
        "#convert emojis into text\n",
        "def convert_emojis(x):\n",
        "    for emot in UNICODE_EMO:\n",
        "        x = x.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
        "    return x\n",
        "\n",
        "data_emotion_test.content_clean = data_emotion_test.content_clean.apply(lambda x: convert_emojis(x))\n",
        "data_emotion_train.content_clean = data_emotion_train.content_clean.apply(lambda x: convert_emojis(x))\n",
        "data_emotion_val.content_clean = data_emotion_val.content_clean.apply(lambda x: convert_emojis(x))\n",
        "\n",
        "#Check\n",
        "print(data_emotion_test.loc[[105]])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                     content                                      content_clean\n",
            "105  @user Wise you mean? ðŸ˜…   @user wise you mean? smiling_face_with_open_mo...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NCzNhj91Wfh"
      },
      "source": [
        "##Remove unnecessary punctuation\n",
        "\n",
        "Certain types of punctuation is not of particular use and is removed using the script below. \n",
        "\n",
        "However, commas, periods, exclamation marks, question marks apostrophes have not been taken out as they help set the tone or define the relationships between words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGlUUihvFZbB"
      },
      "source": [
        "#Remove unnecessary punctuation\n",
        "def punctuation(x): \n",
        "  \n",
        "    punctuations = '()-[]{};:\\<>/#$%^&_~'\n",
        "  \n",
        "    for i in x.lower(): \n",
        "        if i in punctuations: \n",
        "            x = x.replace(i, \" \") \n",
        "    return x\n",
        "\n",
        "data_emotion_test.content_clean = data_emotion_test.content_clean.apply(lambda x: punctuation(x))\n",
        "data_emotion_train.content_clean = data_emotion_train.content_clean.apply(lambda x: punctuation(x))\n",
        "data_emotion_val.content_clean = data_emotion_val.content_clean.apply(lambda x: punctuation(x))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwydchOn2GL8"
      },
      "source": [
        "##Remove '@user' mentions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFNXmcBvP9Ul",
        "outputId": "062fb2e2-612f-419a-84a9-fffbb9512c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Remove @user mentions\n",
        "data_emotion_test.content_clean = data_emotion_test.content_clean.str.replace('@user','')\n",
        "data_emotion_train.content_clean = data_emotion_train.content_clean.str.replace('@user','')\n",
        "data_emotion_val.content_clean = data_emotion_val.content_clean.str.replace('@user','')\n",
        "\n",
        "\n",
        "data_emotion_test.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>content_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#Deppression is real. Partners w/ #depressed p...</td>\n",
              "      <td>deppression is real. partners with   depresse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@user Interesting choice of words... Are you c...</td>\n",
              "      <td>interesting choice of words... are you confir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>My visit to hospital for care triggered #traum...</td>\n",
              "      <td>my visit to hospital for care triggered  traum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@user Welcome to #MPSVT! We are delighted to h...</td>\n",
              "      <td>welcome to  mpsvt! we are delighted to have y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What makes you feel #joyful?</td>\n",
              "      <td>what makes you feel  joyful?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content                                      content_clean\n",
              "0  #Deppression is real. Partners w/ #depressed p...   deppression is real. partners with   depresse...\n",
              "1  @user Interesting choice of words... Are you c...   interesting choice of words... are you confir...\n",
              "2  My visit to hospital for care triggered #traum...  my visit to hospital for care triggered  traum...\n",
              "3  @user Welcome to #MPSVT! We are delighted to h...   welcome to  mpsvt! we are delighted to have y...\n",
              "4                      What makes you feel #joyful?                       what makes you feel  joyful? "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}